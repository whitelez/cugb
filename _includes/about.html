<div class="container">
  <div class="user-details">
    <h3> Summary</h3>
    <p align="left"> CuDB is a parallel implementation of gradient boosting decision tree in CUDA on the GPU. The main focus of the project includes:</p>
    <div align="left"> 
      <ul>
        <li>Developing a sequential implementation of gradient boosting to serve as a baseline.</li>
        <li>Experimenting with different approaches to optimize parallel implementation in Cuda on GPU.</li>
        <li>Comparing and analyzing performance of different implementations against baseline sequential version to optimize our algorithm.
</li>
      </ul>
    </div>
    <h3> Background </h3>
    <h4> Decision Tree </h4>
    <p align="left"> In machine learning, a decision tree is a structure used as a predictive model 
      to conclude about the itemâ€™s target value from a set of attributes. A classification tree takes a finite 
      set of values that represent categories, whereas a regression tree can take continuous values to represent 
      probability. An example of regression tree is shown below in <i>Figure 1</i>. Each node of the decision tree 
      represents a test on a specific attribute such as age and gender, and the outcome of the result is represented 
      by the branch of the node.  The leaf of the tree represents the corresponding target value assigned to the data entries that travel from the root to this particular leaf following the outcomes of testing the attributes. 
</p>
    <div><img src="{{ "/assets/img/img_1.png" | prepend: site.baseurl }}" width="50%" height="50%"/><p>Figure 1: Decision tree example</p></div>
    
    <h4> Learning a Decision Tree </h4>
    <p align="left"> Different algorithms can be used to learn a decision tree from training data set. CuDB uses ID3 learning to construct decision trees.
      The pseudocode is shown below in <i>Figure 2</i>. In each iteration, it traverses the set of unvisited attributes and choose select the best 
      attribute that provides the most information. This is achieved by calculating the 
      <a class="link" target="_blank" href="http://www.cs.cmu.edu/~roni/10601-slides/ch3.pdf">entropy</a> 
      and choose the attribute with the largest information gain. Then it splits the data according to the selected attribute, 
      and recurse on each of the subset. </p>
    
    <div><img src="{{ "/assets/img/id3.png" | prepend: site.baseurl }}" width="50%" height="50%"/><p>Figure 2: ID3 pseudocode</p></div>
   
    <h4> Gradient Boosting </h4>
    
    <p align="left"> A single decision tree is a weak predictor and has its limitations. Gradient boosting is the process of 
      building multiple trees, combining weak predictors to form a strong predicted model. The intuition behind it is that the 
      next iteration corrects the previous error in order to improve overall reliability. The predict result is a weighted 
      combination of each layer of the trees. A simple example is shown below in <i>Figure 3</i>.</p>
    <div><img src="{{ "/assets/img/gb_graph.png" | prepend: site.baseurl }}" width="80%" height="80%"/><p>Figure 3: Gradient boosting example</p></div>
 
    <p align="left"> The general steps of gradient boosting is first construct a naive initial tree. Then for each iteration, 
      we construct a new tree to best reduce error using gradient of the loss function. We calculate a weight &Gamma 
      for the prediction result of this tree by minimizing the new loss. The final prediction output would be the weighted 
      sum of results obtained in all iterations.</p>
    
    <div><img src="{{ "/assets/img/gradient_boosting.png" | prepend: site.baseurl }}" width="80%" height="80%"/><p>Figure 4: Gradient boosting pseudocode</p></div>
 
  </div>
</div>
