<div class="container">
  <div class="user-details">
    <h3> Summary</h3>
    <p align="left"> CuDB is a parallel implementation of gradient boosting decision tree in CUDA on the GPU. The main focus of the project includes:</p>
    <div align="left"> 
      <ul>
        <li>Developing a sequential implementation of gradient boosting to serve as a baseline.</li>
        <li>Experimenting with different approaches to optimize parallel implementation in Cuda on GPU.</li>
        <li>Comparing and analyzing performance of different implementations against baseline sequential version to optimize our algorithm.
</li>
      </ul>
    </div>
    <h3> Background </h3>
    <h5> Decision Tree </h5>
    <p align="left"> In machine learning, a decision tree is a structure used as a predictive model 
      to conclude about the item’s target value from a set of attributes. A classification tree takes a finite 
      set of values that represent categories, whereas a regression tree can take continuous values to represent 
      probability. An example of regression tree is shown below in <i>Figure 1</i>. Each node of the decision tree 
      represents a test on a specific attribute such as age and gender, and the outcome of the result is represented 
      by the branch of the node.  The leaf of the tree represents the corresponding target value assigned to the data entries that travel from the root to this particular leaf following the outcomes of testing the attributes. 
</p>
    <div><img src="{{ "/assets/img/img_1.png" | prepend: site.baseurl }}" width="50%" height="50%"/><p>Figure 1: Decision tree example</p></div>
    
    <h5> Learning a Decision Tree </h5>
    <p align="left"> Different algorithms can be used to learn a decision tree from training data set. CuDB uses ID3 learning to construct decision trees.
      The pseudocode is shown below in <i>Figure 2</i>. In each iteration, it traverses the set of unvisited attributes and choose select the best 
      attribute that provides the most information. This is achieved by calculating the 
      <a class="link" target="_blank" href="http://www.cs.cmu.edu/~roni/10601-slides/ch3.pdf">entropy</a> 
      and choose the attribute with the largest information gain. Then it splits the data according to the selected attribute, 
      and recurse on each of the subset. </p>
    
    <div><img src="{{ "/assets/img/id3.png" | prepend: site.baseurl }}" width="50%" height="50%"/><p>Figure 2: ID3 pseudocode</p></div>
   
    <h5> Gradient Boosting </h5>
    
    <p align="left"> A single decision tree is a weak predictor and has its limitations. Gradient boosting is the process of 
      building multiple trees, combining weak predictors to form a strong predicted model. The intuition behind it is that the 
      next iteration corrects the previous error in order to improve overall reliability. The predict result is a weighted 
      combination of each layer of the trees. A simple example is shown below in <i>Figure 3</i>.</p>
    <div><img src="{{ "/assets/img/gb_graph.png" | prepend: site.baseurl }}" width="80%" height="80%"/><p>Figure 3: Gradient boosting example</p></div>
 
    <p align="left"> The general steps of gradient boosting is first construct a naive initial tree. Then for each iteration, 
      we construct a new tree to best reduce error using gradient of the loss function. We calculate a weight &Gamma;
      for the prediction result of this tree by minimizing the new loss. The final prediction output would be the weighted 
      sum of results obtained in all iterations.</p>
    
    <div><img src="{{ "/assets/img/gradient_boosting.png" | prepend: site.baseurl }}" width="60%" height="60%"/><p>Figure 4: Gradient boosting pseudocode</p></div>
 
    <p align="left">We identified some challenges with gradient boosting. First, a sequential version itself is hard to implement, 
      requiring deep understanding of machine learning and math. We decided to use our implementation as baseline. 
      Second, we need to identify potential places to paralize. The last concern is the reprentation of data set, 
      as in real world application it is quite large. </p>
    
    <h3>Preliminary Requirements</h3>
    <h5>Platform</h5>
    <p align="left">The benchmark result is obtained by running the code on the following platforms:</p>
    <div align="left"> 
      <ul>
        <li>The sequential version is run on machines containing 8 core 3.2 GHz Intel Core i7 processors.</li>
        <li>The Cuda version is run on machines containing <a class="link" target="_blank" href="https://www.nvidia.com/en-us/geforce/products/10series/geforce-gtx-1080/">NVIDIA GeForce GTX 1080</a> . </li>
      </ul>
    </div>  
    
    <h5>Data Set Assumptions</h5>
    
    <p align="left">We obtained training and testing data sets from open source project 
      <a class="link" target="_blank" href="https://github.com/dmlc/xgboost/">xgboost</a>. We slightly modified the original dataset to 
      fit our needs. The largest training dataset contains 1 million entries with 130 attributes. The following are 2 
      requirements on the dataset: </p>
    <div align="left"> 
      <ul>
        <li>All attributes only take binary values, either it’s present or not. The response variable takes 1 and -1.</li>
        <li>And the data set is sparse, meaning that less than half attributes are present for each data entry.</li>
      </ul>
    </div>  
    
    <h3>Approach</h3>
    <h5>Sequential Vesion</h5>
    
    <p align="left">We first construct a sequential version based on the Friedman’s 1999 paper of gradient boosting tree, 
      for our dataset is a binary value attribute dataset and our model is expected to be a binary classifier. 
      This particular model has been discussed in Friedman’s paper. In its paper, Friedman proposed one-step Newton-Raphson 
      approximation of the linear optimization of the loss function. We directly implemented that approximation for it would 
      take less time than computing the whole gradient of all possible outcome and select the best one from it. 

      The construction algorithm of decision tree is based on pseudo-code provided by 10-701 course, and we are using entropy 
      approach (ID3 algorithm) to select the best attribute from whole dataset in each iteration. 

    
  </div>
</div>
